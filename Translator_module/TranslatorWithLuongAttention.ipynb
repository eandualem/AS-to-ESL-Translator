{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"TranslatorWithAttention1.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-I5FuBphquhu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596027395489,"user_tz":-180,"elapsed":1280,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["import translator_helper as helper\n","import os\n","import preprocess_util as putil\n","source_path = \"./raw_data/parallel_sentence_corpus/amh2.txt\"\n","target_path = \"./raw_data/parallel_sentence_corpus/amh2.txt\"\n","source_vocab_mapping = \"./amh_vocab_mapping.p\"\n","target_vocab_mapping = \"./amh_vocab_mapping.p\"\n","PREPROCESS_SAVE_PATH = \"preprocssed_data.p\"\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"sPXFam88KNrm","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HBEySQcWvRsR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596022405365,"user_tz":-180,"elapsed":44095,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["# !ln -s ./drive/My\\ Drive/HornMorph/\n","# !ln -s ./drive/My\\ Drive/AMh2ESLTest/raw_data/\n","# !ln -s ./drive/My\\ Drive/AMh2ESLTest/processed_data/\n","# !ln -s ./drive/My\\ Drive/AMh2ESLTest/embedding_checkpoints/\n","# !ln -s ./drive/My\\ Drive/AMh2ESLTest/translator_helper.py\n","# !ln -s ./drive/My\\ Drive/AMh2ESLTest/preprocess_util.py\n","# !ln -s ./drive/My\\ Drive/AMh2ESLTest/word2vec_helper.py\n","# !ln -s ./drive/My\\ Drive/AMh2ESLTest/amh_vocab_mapping.py\n","# !ln -s ./drive/My\\ Drive/AMh2ESLTest/amh_vocab_mapping.p\n","# !ln -s ./drive/My\\ Drive/AMh2ESLTest/preprocssed_data.p\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hKBrWLYquhz","colab_type":"code","colab":{}},"source":["src_int_to_vocab, src_vocab_to_int = putil.load_file(source_vocab_mapping)\n","tgt_int_to_vocab, tgt_vocab_to_int = putil.load_file(target_vocab_mapping)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"laRKfugpquh4","colab_type":"code","colab":{}},"source":["helper.preprocess_and_save(source_path, target_path,\n","                           src_int_to_vocab, src_vocab_to_int,\n","                           tgt_int_to_vocab, tgt_vocab_to_int,\n","                           PREPROCESS_SAVE_PATH)\n","print(\"processed data saved...\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vziHeYrtquh7","colab_type":"code","colab":{}},"source":["#checkpoint processed data saved"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NoAbnRnJquh-","colab_type":"code","colab":{}},"source":["import translator_helper as helper\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import preprocess_util as putil\n","import numpy as np\n","PREPROCESS_SAVE_PATH = \"preprocssed_data.p\"\n","(src_int_text, tgt_int_text), (src_int_to_vocab, tgt_int_to_vocab), (src_vocab_to_int, tgt_vocab_to_int) = helper.load_preprocessed_data(PREPROCESS_SAVE_PATH)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVISvUVLquiC","colab_type":"code","colab":{}},"source":["from tensorflow.python.layers.core import Dense\n","\n","print(tf.__version__)\n","\n","if not tf.test.gpu_device_name():\n","    print('No GPU found. Please use a GPU to train your neural network.')\n","else:\n","    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bX_U5mcLbqJr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596027409865,"user_tz":-180,"elapsed":2318,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["def get_embedding_mat(embedding_size, vocab_size, load_path):\n","  embed_graph = tf.Graph()\n","  with embed_graph.as_default():\n","    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n","    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n","    embedding = tf.Variable(tf.truncated_normal([vocab_size, embedding_size]))\n","  with embed_graph.as_default():\n","    saver = tf.train.Saver()\n","\n","  with tf.Session(graph=embed_graph) as sess:\n","    saver.restore(sess, tf.train.latest_checkpoint(load_path))\n","    embed_mat = sess.run(embedding)\n","  #tf.reset_default_graph()\n","  return embed_mat"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"vbcRH_xOquiE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596027409866,"user_tz":-180,"elapsed":2307,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["# prepare_model_input\n","def model_inputs():\n","    src_input = tf.placeholder(tf.int32, (None, None), name=\"src_input\")\n","    target = tf.placeholder(tf.int32, (None, None), name=\"targets\")\n","    learning_rate = tf.placeholder(tf.float32, [], name=\"learning_rate\")\n","    keep_prob = tf.placeholder(tf.float32, [], name=\"keep_prob\")\n","    src_seq_len = tf.placeholder(tf.int32, (None, ), name=\"src_seq_len\")\n","    tgt_seq_len = tf.placeholder(tf.int32, (None, ), name=\"tgt_seq_len\")\n","    max_tgt_seq = tf.reduce_max(tgt_seq_len)\n","    \n","    return src_input, target, learning_rate, keep_prob, src_seq_len, tgt_seq_len, max_tgt_seq"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEVYv4r8quiI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596027409866,"user_tz":-180,"elapsed":2289,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["def prepare_decoder_input(target, target_to_int, batch_size):\n","    sliced = tf.strided_slice(target, [0,0], [batch_size, -1], [1,1])\n","    decoder_input = tf.concat([tf.fill([batch_size,1], target_to_int['<GO>']), sliced], 1)\n","    return decoder_input\n","    "],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"mL0Ppb4GquiK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596027409867,"user_tz":-180,"elapsed":2280,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["def encoder(enc_inputs, src_seq_len, enc_embedding_size, src_vocab_size, rnn_size, num_layers, keep_prob=0.5, embed_mat=None):\n","\n","    #embed = tf.contrib.layers.embed_sequence(enc_inputs, src_vocab_size, enc_embedding_size)\n","    #embed_mat = get_embedding_mat(enc_embedding_size, src_vocab_size, src_embed_load_path)\n","    embed = tf.nn.embedding_lookup(embed_mat, enc_inputs)\n","\n","    def build_cell(lstm_size, keep_prob):\n","        lstm = tf.contrib.rnn.LSTMCell(lstm_size)\n","        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n","        return drop\n","    cell = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n","    \n","    # returns output and state\n","    outputs, state = tf.nn.dynamic_rnn(cell, embed, sequence_length=src_seq_len, dtype=tf.float32)\n","    return outputs, state   "],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"F48mjJgxquiN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596027411050,"user_tz":-180,"elapsed":3453,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["def decoder_train(dec_embeded_input, enc_state, tgt_seq_len, max_tgt_len, dec_cell, keep_prob, output_layer):\n","    \n","    train_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embeded_input, \n","                                                     sequence_length=tgt_seq_len, \n","                                                     time_major=False)\n","    # dec_train = tf.contrib.seq2seq.BasicDecoder(dec_cell, train_helper, enc_state, output_layer)\n","    dec_train = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell, helper=train_helper, initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size), output_layer=output_layer) \n","    decoder_train_output = tf.contrib.seq2seq.dynamic_decode(dec_train,\n","                                                       impute_finished=True,\n","                                                       maximum_iterations=max_tgt_len)[0]\n","    return decoder_train_output\n","    "],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"YE7akRM2quiP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596027411050,"user_tz":-180,"elapsed":3444,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["def decoder_inference(dec_embedding_mat, enc_state, max_tgt_len, start_id, end_id, batch_size, dec_cell, keep_prob, output_layer):\n","    \n","    start_tokens = tf.tile(tf.constant([start_id], dtype=tf.int32), [batch_size], name='start_tokens')\n","    \n","    infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embedding_mat, start_tokens, end_id)\n","    \n","    dec_inference = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell, helper=infer_helper, initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size), output_layer=output_layer)\n","    \n","    decoder_infer_output = tf.contrib.seq2seq.dynamic_decode(dec_inference,\n","                                                       impute_finished=True,\n","                                                       maximum_iterations=max_tgt_len)[0]\n","    return decoder_infer_output\n","    "],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"uIDKr2jHquiR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596027411051,"user_tz":-180,"elapsed":3436,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["def decoder(dec_input, enc_state, enc_output, src_seq_len,\n","            tgt_seq_len, max_tgt_len, \n","            tgt_vocab_to_int, tgt_vocab_size, \n","            rnn_size, num_layers, dec_embed_size,\n","            batch_size, keep_prob, embed_mat=None):\n","    #embed_mat = tf.Variable(tf.random_uniform([tgt_vocab_size, dec_embed_size]))\n","    #embed_mat = get_embedding_mat(enc_embedding_size, tgt_vocab_size, tgt_embed_load_path)\n","    dec_embeded_input = tf.nn.embedding_lookup(embed_mat, dec_input)\n","    \n","    def build_cell(lstm_size, keep_prob):\n","        lstm = tf.contrib.rnn.LSTMCell(lstm_size)\n","        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n","        return drop\n","    \n","    dec_cell = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n","    attention_states = enc_output\n","    attention_mechanism = tf.contrib.seq2seq.LuongAttention(rnn_size, attention_states, memory_sequence_length=src_seq_len)\n","    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism, attention_layer_size=rnn_size)\n","    \n","    output_layer = Dense(tgt_vocab_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n","    \n","    with tf.variable_scope(\"decode\"):\n","        dec_train_output = decoder_train(dec_embeded_input, enc_state,\n","                                         tgt_seq_len, max_tgt_len,\n","                                         dec_cell, keep_prob, output_layer)\n","    start_id = tgt_vocab_to_int[\"<GO>\"]\n","    end_id = tgt_vocab_to_int[\"<EOS>\"]\n","   \n","    with tf.variable_scope(\"decode\", reuse=True):\n","        dec_infer_output = decoder_inference(embed_mat, enc_state, max_tgt_len,\n","                                                start_id, end_id, batch_size,\n","                                                dec_cell, keep_prob, output_layer)\n","    return dec_train_output, dec_infer_output\n","    "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Vdr4agZquiV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596027411052,"user_tz":-180,"elapsed":3427,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["def EncoderDecoder(inputs, targets,\n","                   enc_embedding_size, dec_embedding_size,\n","                   src_seq_len, tgt_seq_len, max_tgt_len, \n","                   tgt_vocab_size, src_vocab_size, tgt_vocab_to_int,\n","                   rnn_size, num_layers, batch_size, keep_prob,\n","                   src_embed_mat, tgt_embed_mat):\n","    \n","    outputs, state = encoder(inputs, src_seq_len, enc_embedding_size,\n","                      src_vocab_size, rnn_size, num_layers, keep_prob, src_embed_mat)\n","    \n","    dec_procssed_input = prepare_decoder_input(targets, tgt_vocab_to_int, batch_size)\n","    \n","    decoder_train_output, dec_infer_output = decoder(dec_procssed_input, state, outputs,src_seq_len, tgt_seq_len, max_tgt_len,\n","                                                      tgt_vocab_to_int, tgt_vocab_size, rnn_size, num_layers,\n","                                                      dec_embedding_size, batch_size, keep_prob,tgt_embed_mat)\n","    return decoder_train_output, dec_infer_output"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"XjJ9y21vquiX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596027411052,"user_tz":-180,"elapsed":3390,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["def pad_sentence_batch(sentence_batch, pad_int):\n","    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n","    max_sentence = max([len(sentence) for sentence in sentence_batch])\n","    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n","    "],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZAVv3mtquia","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596027411053,"user_tz":-180,"elapsed":3379,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n","    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n","    for batch_i in range(0, len(sources)//batch_size):\n","        start_i = batch_i * batch_size\n","\n","        # Slice the right amount for the batch\n","        sources_batch = sources[start_i:start_i + batch_size]\n","        targets_batch = targets[start_i:start_i + batch_size]\n","\n","        # Pad\n","        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n","        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n","\n","        # Need the lengths for the _lengths parameters\n","        pad_targets_lengths = []\n","        for target in pad_targets_batch:\n","            pad_targets_lengths.append(len(target))\n","\n","        pad_source_lengths = []\n","        for source in pad_sources_batch:\n","            pad_source_lengths.append(len(source))\n","\n","        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n","\n","        "],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lco2tnYequic","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596027411053,"user_tz":-180,"elapsed":3358,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["tf.reset_default_graph()\n","train_graph = tf.Graph()\n","(src_int_text, tgt_int_text), (src_vocab_to_int, tgt_vocab_to_int), (src_int_to_vocab, tgt_int_to_vocab)  = helper.load_preprocessed_data(PREPROCESS_SAVE_PATH)\n","max_tgt_len = max([len(sen) for sen in tgt_int_text])"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwoeYF0Nquif","colab_type":"code","colab":{}},"source":["epochs = 100\n","batch_size = 512\n","lr = 0.001\n","keep_probe = 0.75\n","rnn_size = 256\n","num_layers = 2\n","enc_embedding_size = 256\n","dec_embedding_size = 256\n","display_step = 10\n","src_embed_mat = get_embedding_mat(256, len(src_int_to_vocab),\"embedding_checkpoints\")\n","tgt_embed_mat = get_embedding_mat(256, len(src_int_to_vocab),\"embedding_checkpoints\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sehzwaXTquih","colab_type":"code","colab":{}},"source":["\n","with train_graph.as_default():\n","    src_inputs, targets, learning_rate, keep_prob, src_seq_len, tgt_seq_len, max_tgt_seq = model_inputs()\n","    print(\"F\")\n","    train_logits, infer_logits = EncoderDecoder(src_inputs, targets,\n","                                                enc_embedding_size, dec_embedding_size,\n","                                                src_seq_len, tgt_seq_len, max_tgt_len, \n","                                                len(src_vocab_to_int), len(tgt_vocab_to_int), tgt_vocab_to_int,\n","                                                rnn_size, num_layers, batch_size, keep_prob,\n","                                                src_embed_mat, tgt_embed_mat)\n","    \n","    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n","    print(\"G\")\n","    inference_logits = tf.identity(infer_logits.sample_id, name='predictions')\n","    masks = tf.sequence_mask(tgt_seq_len, max_tgt_seq, dtype=tf.float32, name='masks')\n","    \n","    with tf.name_scope(\"optimization\"):\n","        # Loss function\n","        cost = tf.contrib.seq2seq.sequence_loss(\n","            training_logits,\n","            targets,\n","            masks)\n","\n","        # Optimizer\n","        optimizer = tf.train.AdamOptimizer(learning_rate)\n","\n","        # Gradient Clipping\n","        gradients = optimizer.compute_gradients(cost)\n","        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n","        train_optimzer = optimizer.apply_gradients(capped_gradients)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HoEAFyZdo1HB","colab_type":"code","colab":{}},"source":["#!mkdir ./translating_checkpoints2/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Yqzkotgquil","colab_type":"code","colab":{}},"source":["save_path = \"./translating_checkpoints2/dev\"\n","with tf.Session(graph=train_graph) as sess:\n","    sess.run(tf.global_variables_initializer())\n","    src_pad_id = src_vocab_to_int['<PAD>']\n","    tgt_pad_id = tgt_vocab_to_int['<PAD>']\n","    #batches = getBatches(src_int_text, tgt_int_text, batch_size, src_pad_id, tgt_pad_id)\n","    for epoch_i in range(epochs):\n","        for batch_ind, (src_batch,tgt_batch,srce_seq_length,tgte_seq_length) in enumerate(\n","            get_batches(src_int_text, tgt_int_text, batch_size, src_pad_id, tgt_pad_id)):\n","            #srce_seq_length = [len(sent) for sent in src_batch]\n","            #tgte_seq_length = [len(sent) for sent in tgt_batch]\n","            _, loss = sess.run(\n","                [train_optimzer, cost],\n","                {src_inputs: src_batch,\n","                 targets: tgt_batch,\n","                 learning_rate: lr,\n","                 tgt_seq_len: tgte_seq_length,\n","                 src_seq_len: srce_seq_length,\n","                 keep_prob: keep_probe\n","                })\n","\n","\n","            if batch_ind % display_step == 0 and batch_ind > 0:\n","\n","\n","                batch_train_logits = sess.run(\n","                    inference_logits,\n","                    {src_inputs: src_batch,\n","                     src_seq_len: srce_seq_length,\n","                     tgt_seq_len: tgte_seq_length,\n","                     keep_prob: 1.0})\n","\n","                print('Epoch {:>3} Batch {:>4}/{} - Loss: {:>6.4f}'\n","                      .format(epoch_i, batch_ind, len(src_int_text) // batch_size, loss))\n","\n","    # Save Model\n","    saver = tf.train.Saver()\n","    saver.save(sess, save_path)\n","    print('Model Trained and Saved')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sWMZqHDTQAC8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596029121578,"user_tz":-180,"elapsed":5328,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}}},"source":["#!ln -s ./drive/My\\ Drive/AMh2ESLTest/predict.py\n","#!ln -s  ./drive/My\\ Drive/AMh2ESLTest/translating_checkpoints2/\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zgcEX8mQYlK","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","#from predict import Amh2ESLTranslate\n","import predict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9wzSDFcltKhy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1596029643904,"user_tz":-180,"elapsed":5232,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"outputId":"4aacfb18-d0e4-4499-9b40-7cd15f0197ac"},"source":["predict.translate(\"እርስዋን በመንገድህ ባሪያህን በቃልህ እጅግ አድርገኝ\", \"./translating_checkpoints2/dev\")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./translating_checkpoints2/dev\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'እርስዋን ወድጃለሁና የትእዛዝህን መንገድ ምራኝ <EOS>'"]},"metadata":{"tags":[]},"execution_count":12}]}]}