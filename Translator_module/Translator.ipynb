{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Translator.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-I5FuBphquhu","colab_type":"code","outputId":"aa4eede4-b085-4dfe-ec52-65c705529990","executionInfo":{"status":"ok","timestamp":1590577555414,"user_tz":-180,"elapsed":14078,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import translator_helper as helper\n","import os\n","import preprocess_util as putil\n","source_path = \"./raw_data/parallel_sentence_corpus/amh.txt\"\n","target_path = \"./raw_data/parallel_sentence_corpus/amh.txt\"\n","source_vocab_mapping = \"./amh_vocab_mapping.p\"\n","target_vocab_mapping = \"./amh_vocab_mapping.p\"\n","PREPROCESS_SAVE_PATH = \"preprocssed_data.p\"\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n",">>>>> This is L3Morpho, version 3.0 <<<<<\n",">>>>>  and HornMorpho, version 2.5  <<<<<\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HBEySQcWvRsR","colab_type":"code","colab":{}},"source":["#!ln -s ./drive/My\\ Drive/HornMorph\n","#!ln -s ./drive/My\\ Drive/Amh2Amh_v3_checked/* ./"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hKBrWLYquhz","colab_type":"code","colab":{}},"source":["src_int_to_vocab, src_vocab_to_int = putil.load_file(source_vocab_mapping)\n","tgt_int_to_vocab, tgt_vocab_to_int = putil.load_file(target_vocab_mapping)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"laRKfugpquh4","colab_type":"code","outputId":"b2f0a37b-8b9d-471d-a7dd-03d47827e4a2","executionInfo":{"status":"ok","timestamp":1590577566905,"user_tz":-180,"elapsed":1935,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["helper.preprocess_and_save(source_path, target_path,\n","                           src_int_to_vocab, src_vocab_to_int,\n","                           tgt_int_to_vocab, tgt_vocab_to_int,\n","                           PREPROCESS_SAVE_PATH)\n","print(\"processed data saved...\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["processed data saved...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vziHeYrtquh7","colab_type":"code","outputId":"ab82fa3b-f886-4930-8857-760599bbfe0b","executionInfo":{"status":"ok","timestamp":1590509076403,"user_tz":-180,"elapsed":959,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#checkpoint processed data saved"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["39770"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"NoAbnRnJquh-","colab_type":"code","outputId":"554c10e0-da0e-4689-8c64-767cc8fa15b7","executionInfo":{"status":"ok","timestamp":1590581307260,"user_tz":-180,"elapsed":2546,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import translator_helper as helper\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import preprocess_util as putil\n","import numpy as np\n","PREPROCESS_SAVE_PATH = \"preprocssed_data.p\"\n","(src_int_text, tgt_int_text), (src_int_to_vocab, tgt_int_to_vocab), (src_vocab_to_int, tgt_vocab_to_int) = helper.load_preprocessed_data(PREPROCESS_SAVE_PATH)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n",">>>>> This is L3Morpho, version 3.0 <<<<<\n",">>>>>  and HornMorpho, version 2.5  <<<<<\n","TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OVISvUVLquiC","colab_type":"code","outputId":"a3863831-019f-4a88-eccd-b9f1592d29e3","executionInfo":{"status":"ok","timestamp":1590581311085,"user_tz":-180,"elapsed":1615,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from tensorflow.python.layers.core import Dense\n","\n","print(tf.__version__)\n","\n","if not tf.test.gpu_device_name():\n","    print('No GPU found. Please use a GPU to train your neural network.')\n","else:\n","    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.15.2\n","Default GPU Device: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vbcRH_xOquiE","colab_type":"code","colab":{}},"source":["# prepare_model_input\n","def model_inputs():\n","    src_input = tf.placeholder(tf.int32, (None, None), name=\"src_input\")\n","    target = tf.placeholder(tf.int32, (None, None), name=\"targets\")\n","    learning_rate = tf.placeholder(tf.float32, [], name=\"learning_rate\")\n","    keep_prob = tf.placeholder(tf.float32, [], name=\"keep_prob\")\n","    src_seq_len = tf.placeholder(tf.int32, (None, ), name=\"src_seq_len\")\n","    tgt_seq_len = tf.placeholder(tf.int32, (None, ), name=\"tgt_seq_len\")\n","    max_tgt_seq = tf.reduce_max(tgt_seq_len)\n","    \n","    return src_input, target, learning_rate, keep_prob, src_seq_len, tgt_seq_len, max_tgt_seq"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEVYv4r8quiI","colab_type":"code","colab":{}},"source":["def prepare_decoder_input(target, target_to_int, batch_size):\n","    sliced = tf.strided_slice(target, [0,0], [batch_size, -1], [1,1])\n","    decoder_input = tf.concat([tf.fill([batch_size,1], target_to_int['<GO>']), sliced], 1)\n","    return decoder_input\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mL0Ppb4GquiK","colab_type":"code","colab":{}},"source":["def encoder(enc_inputs, src_seq_len, enc_embedding_size, src_vocab_size, rnn_size, num_layers, keep_prob=0.5):\n","    embed = tf.contrib.layers.embed_sequence(enc_inputs, src_vocab_size, enc_embedding_size)\n","    def build_cell(lstm_size, keep_prob):\n","        lstm = tf.contrib.rnn.LSTMCell(lstm_size)\n","        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n","        return drop\n","    cell = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n","    \n","    # returns output and state\n","    outputs, state = tf.nn.dynamic_rnn(cell, embed, sequence_length=src_seq_len, dtype=tf.float32)\n","    return outputs, state   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F48mjJgxquiN","colab_type":"code","colab":{}},"source":["def decoder_train(dec_embeded_input, enc_state, tgt_seq_len, max_tgt_len, dec_cell, keep_prob, output_layer):\n","    \n","    train_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embeded_input, \n","                                                     sequence_length=tgt_seq_len, \n","                                                     time_major=False)\n","    dec_train = tf.contrib.seq2seq.BasicDecoder(dec_cell, train_helper, enc_state, output_layer)\n","    \n","    decoder_train_output = tf.contrib.seq2seq.dynamic_decode(dec_train,\n","                                                       impute_finished=True,\n","                                                       maximum_iterations=max_tgt_len)[0]\n","    return decoder_train_output\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YE7akRM2quiP","colab_type":"code","colab":{}},"source":["def decoder_inference(dec_embedding_mat, enc_state, max_tgt_len, start_id, end_id, batch_size, dec_cell, keep_prob, output_layer):\n","    \n","    start_tokens = tf.tile(tf.constant([start_id], dtype=tf.int32), [batch_size], name='start_tokens')\n","    \n","    infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embedding_mat, start_tokens, end_id)\n","    \n","    dec_inference = tf.contrib.seq2seq.BasicDecoder(dec_cell, infer_helper, enc_state, output_layer)\n","    \n","    decoder_infer_output = tf.contrib.seq2seq.dynamic_decode(dec_inference,\n","                                                       impute_finished=True,\n","                                                       maximum_iterations=max_tgt_len)[0]\n","    return decoder_infer_output\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uIDKr2jHquiR","colab_type":"code","colab":{}},"source":["def decoder(dec_input, enc_state, \n","            tgt_seq_len, max_tgt_len, \n","            tgt_vocab_to_int, tgt_vocab_size, \n","            rnn_size, num_layers, dec_embed_size,\n","            batch_size, keep_prob):\n","    embed_mat = tf.Variable(tf.random_uniform([tgt_vocab_size, dec_embed_size]))\n","    dec_embeded_input = tf.nn.embedding_lookup(embed_mat, dec_input)\n","    \n","    def build_cell(lstm_size, keep_prob):\n","        lstm = tf.contrib.rnn.LSTMCell(lstm_size)\n","        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n","        return drop\n","    \n","    dec_cell = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n","    output_layer = Dense(tgt_vocab_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n","    \n","    with tf.variable_scope(\"decode\"):\n","        dec_train_output = decoder_train(dec_embeded_input, enc_state,\n","                                         tgt_seq_len, max_tgt_len,\n","                                         dec_cell, keep_prob, output_layer)\n","    start_id = tgt_vocab_to_int[\"<GO>\"]\n","    end_id = tgt_vocab_to_int[\"<EOS>\"]\n","   \n","    with tf.variable_scope(\"decode\", reuse=True):\n","        dec_infer_output = decoder_inference(embed_mat, enc_state, max_tgt_len,\n","                                                start_id, end_id, batch_size,\n","                                                dec_cell, keep_prob, output_layer)\n","    return dec_train_output, dec_infer_output\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Vdr4agZquiV","colab_type":"code","colab":{}},"source":["def EncoderDecoder(inputs, targets,\n","                   enc_embedding_size, dec_embedding_size,\n","                   src_seq_len, tgt_seq_len, max_tgt_len, \n","                   tgt_vocab_size, src_vocab_size, tgt_vocab_to_int,\n","                   rnn_size, num_layers, batch_size, keep_prob):\n","    \n","    outputs, state = encoder(inputs, src_seq_len, enc_embedding_size,\n","                      src_vocab_size, rnn_size, num_layers, keep_prob)\n","    \n","    dec_procssed_input = prepare_decoder_input(targets, tgt_vocab_to_int, batch_size)\n","    \n","    decoder_train_output, dec_infer_output = decoder(dec_procssed_input, state, tgt_seq_len, max_tgt_len,\n","                                                      tgt_vocab_to_int, tgt_vocab_size, rnn_size, num_layers,\n","                                                      dec_embedding_size, batch_size, keep_prob)\n","    return decoder_train_output, dec_infer_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XjJ9y21vquiX","colab_type":"code","colab":{}},"source":["def pad_sentence_batch(sentence_batch, pad_int):\n","    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n","    max_sentence = max([len(sentence) for sentence in sentence_batch])\n","    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZAVv3mtquia","colab_type":"code","colab":{}},"source":["def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n","    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n","    for batch_i in range(0, len(sources)//batch_size):\n","        start_i = batch_i * batch_size\n","\n","        # Slice the right amount for the batch\n","        sources_batch = sources[start_i:start_i + batch_size]\n","        targets_batch = targets[start_i:start_i + batch_size]\n","\n","        # Pad\n","        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n","        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n","\n","        # Need the lengths for the _lengths parameters\n","        pad_targets_lengths = []\n","        for target in pad_targets_batch:\n","            pad_targets_lengths.append(len(target))\n","\n","        pad_source_lengths = []\n","        for source in pad_sources_batch:\n","            pad_source_lengths.append(len(source))\n","\n","        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n","\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lco2tnYequic","colab_type":"code","colab":{}},"source":["train_graph = tf.Graph()\n","(src_int_text, tgt_int_text), (src_vocab_to_int, tgt_vocab_to_int), (src_int_to_vocab, tgt_int_to_vocab)  = helper.load_preprocessed_data(PREPROCESS_SAVE_PATH)\n","max_tgt_len = max([len(sen) for sen in tgt_int_text])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwoeYF0Nquif","colab_type":"code","colab":{}},"source":["epochs = 10\n","batch_size = 50\n","lr = 0.001\n","keep_probe = 0.75\n","rnn_size = 256\n","num_layers = 2\n","enc_embedding_size = 256\n","dec_embedding_size = 256\n","display_step = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sehzwaXTquih","colab_type":"code","colab":{}},"source":["\n","with train_graph.as_default():\n","    src_inputs, targets, learning_rate, keep_prob, src_seq_len, tgt_seq_len, max_tgt_seq = model_inputs()\n","    train_logits, infer_logits = EncoderDecoder(src_inputs, targets,\n","                                                enc_embedding_size, dec_embedding_size,\n","                                                src_seq_len, tgt_seq_len, max_tgt_len, \n","                                                len(src_vocab_to_int), len(tgt_vocab_to_int), tgt_vocab_to_int,\n","                                                rnn_size, num_layers, batch_size, keep_prob)\n","    \n","    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n","    inference_logits = tf.identity(infer_logits.sample_id, name='predictions')\n","    masks = tf.sequence_mask(tgt_seq_len, max_tgt_seq, dtype=tf.float32, name='masks')\n","    \n","    with tf.name_scope(\"optimization\"):\n","        # Loss function\n","        cost = tf.contrib.seq2seq.sequence_loss(\n","            training_logits,\n","            targets,\n","            masks)\n","\n","        # Optimizer\n","        optimizer = tf.train.AdamOptimizer(learning_rate)\n","\n","        # Gradient Clipping\n","        gradients = optimizer.compute_gradients(cost)\n","        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n","        train_optimzer = optimizer.apply_gradients(capped_gradients)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Yqzkotgquil","colab_type":"code","outputId":"bbd2f4b9-e7d5-480d-d7a8-6e398a9f9bf5","executionInfo":{"status":"ok","timestamp":1590581788803,"user_tz":-180,"elapsed":403490,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["save_path = \"./translating_checkpoints/dev\"\n","with tf.Session(graph=train_graph) as sess:\n","    sess.run(tf.global_variables_initializer())\n","    src_pad_id = src_vocab_to_int['<PAD>']\n","    tgt_pad_id = tgt_vocab_to_int['<PAD>']\n","    #batches = getBatches(src_int_text, tgt_int_text, batch_size, src_pad_id, tgt_pad_id)\n","    for epoch_i in range(epochs):\n","        for batch_ind, (src_batch,tgt_batch,srce_seq_length,tgte_seq_length) in enumerate(\n","            get_batches(src_int_text, tgt_int_text, batch_size, src_pad_id, tgt_pad_id)):\n","            #srce_seq_length = [len(sent) for sent in src_batch]\n","            #tgte_seq_length = [len(sent) for sent in tgt_batch]\n","            _, loss = sess.run(\n","                [train_optimzer, cost],\n","                {src_inputs: src_batch,\n","                 targets: tgt_batch,\n","                 learning_rate: lr,\n","                 tgt_seq_len: tgte_seq_length,\n","                 src_seq_len: srce_seq_length,\n","                 keep_prob: keep_probe\n","                })\n","\n","\n","            if batch_ind % display_step == 0 and batch_ind > 0:\n","\n","\n","                batch_train_logits = sess.run(\n","                    inference_logits,\n","                    {src_inputs: src_batch,\n","                     src_seq_len: srce_seq_length,\n","                     tgt_seq_len: tgte_seq_length,\n","                     keep_prob: 1.0})\n","\n","                print('Epoch {:>3} Batch {:>4}/{} - Loss: {:>6.4f}'\n","                      .format(epoch_i, batch_ind, len(src_int_text) // batch_size, loss))\n","\n","    # Save Model\n","    saver = tf.train.Saver()\n","    saver.save(sess, save_path)\n","    print('Model Trained and Saved')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch   0 Batch   10/100 - Loss: 7.8987\n","Epoch   0 Batch   20/100 - Loss: 5.4213\n","Epoch   0 Batch   30/100 - Loss: 4.9766\n","Epoch   0 Batch   40/100 - Loss: 4.4350\n","Epoch   0 Batch   50/100 - Loss: 4.9351\n","Epoch   0 Batch   60/100 - Loss: 5.0886\n","Epoch   0 Batch   70/100 - Loss: 6.1501\n","Epoch   0 Batch   80/100 - Loss: 4.3143\n","Epoch   0 Batch   90/100 - Loss: 4.0865\n","Epoch   1 Batch   10/100 - Loss: 5.1181\n","Epoch   1 Batch   20/100 - Loss: 4.0537\n","Epoch   1 Batch   30/100 - Loss: 4.0796\n","Epoch   1 Batch   40/100 - Loss: 3.7782\n","Epoch   1 Batch   50/100 - Loss: 4.2841\n","Epoch   1 Batch   60/100 - Loss: 4.5916\n","Epoch   1 Batch   70/100 - Loss: 5.6823\n","Epoch   1 Batch   80/100 - Loss: 4.0132\n","Epoch   1 Batch   90/100 - Loss: 3.8081\n","Epoch   2 Batch   10/100 - Loss: 5.0479\n","Epoch   2 Batch   20/100 - Loss: 3.9458\n","Epoch   2 Batch   30/100 - Loss: 3.9750\n","Epoch   2 Batch   40/100 - Loss: 3.6450\n","Epoch   2 Batch   50/100 - Loss: 4.0814\n","Epoch   2 Batch   60/100 - Loss: 4.4565\n","Epoch   2 Batch   70/100 - Loss: 5.3720\n","Epoch   2 Batch   80/100 - Loss: 3.8586\n","Epoch   2 Batch   90/100 - Loss: 3.7067\n","Epoch   3 Batch   10/100 - Loss: 4.9721\n","Epoch   3 Batch   20/100 - Loss: 3.8650\n","Epoch   3 Batch   30/100 - Loss: 3.8974\n","Epoch   3 Batch   40/100 - Loss: 3.5831\n","Epoch   3 Batch   50/100 - Loss: 4.0043\n","Epoch   3 Batch   60/100 - Loss: 4.3421\n","Epoch   3 Batch   70/100 - Loss: 5.2384\n","Epoch   3 Batch   80/100 - Loss: 3.7736\n","Epoch   3 Batch   90/100 - Loss: 3.6249\n","Epoch   4 Batch   10/100 - Loss: 4.9036\n","Epoch   4 Batch   20/100 - Loss: 3.7819\n","Epoch   4 Batch   30/100 - Loss: 3.8372\n","Epoch   4 Batch   40/100 - Loss: 3.5463\n","Epoch   4 Batch   50/100 - Loss: 3.9258\n","Epoch   4 Batch   60/100 - Loss: 4.2573\n","Epoch   4 Batch   70/100 - Loss: 4.9862\n","Epoch   4 Batch   80/100 - Loss: 3.7860\n","Epoch   4 Batch   90/100 - Loss: 3.5526\n","Epoch   5 Batch   10/100 - Loss: 4.7757\n","Epoch   5 Batch   20/100 - Loss: 3.7408\n","Epoch   5 Batch   30/100 - Loss: 3.8044\n","Epoch   5 Batch   40/100 - Loss: 3.5252\n","Epoch   5 Batch   50/100 - Loss: 3.8692\n","Epoch   5 Batch   60/100 - Loss: 4.1820\n","Epoch   5 Batch   70/100 - Loss: 4.8624\n","Epoch   5 Batch   80/100 - Loss: 3.7151\n","Epoch   5 Batch   90/100 - Loss: 3.4941\n","Epoch   6 Batch   10/100 - Loss: 4.7306\n","Epoch   6 Batch   20/100 - Loss: 3.6522\n","Epoch   6 Batch   30/100 - Loss: 3.7535\n","Epoch   6 Batch   40/100 - Loss: 3.4483\n","Epoch   6 Batch   50/100 - Loss: 3.7874\n","Epoch   6 Batch   60/100 - Loss: 4.1335\n","Epoch   6 Batch   70/100 - Loss: 4.6649\n","Epoch   6 Batch   80/100 - Loss: 3.6633\n","Epoch   6 Batch   90/100 - Loss: 3.4855\n","Epoch   7 Batch   10/100 - Loss: 4.6593\n","Epoch   7 Batch   20/100 - Loss: 3.6229\n","Epoch   7 Batch   30/100 - Loss: 3.6878\n","Epoch   7 Batch   40/100 - Loss: 3.3718\n","Epoch   7 Batch   50/100 - Loss: 3.7106\n","Epoch   7 Batch   60/100 - Loss: 4.0927\n","Epoch   7 Batch   70/100 - Loss: 4.5227\n","Epoch   7 Batch   80/100 - Loss: 3.5409\n","Epoch   7 Batch   90/100 - Loss: 3.3975\n","Epoch   8 Batch   10/100 - Loss: 4.5801\n","Epoch   8 Batch   20/100 - Loss: 3.5259\n","Epoch   8 Batch   30/100 - Loss: 3.6023\n","Epoch   8 Batch   40/100 - Loss: 3.3038\n","Epoch   8 Batch   50/100 - Loss: 3.6478\n","Epoch   8 Batch   60/100 - Loss: 3.9901\n","Epoch   8 Batch   70/100 - Loss: 4.4031\n","Epoch   8 Batch   80/100 - Loss: 3.4792\n","Epoch   8 Batch   90/100 - Loss: 3.3305\n","Epoch   9 Batch   10/100 - Loss: 4.5337\n","Epoch   9 Batch   20/100 - Loss: 3.4963\n","Epoch   9 Batch   30/100 - Loss: 3.5343\n","Epoch   9 Batch   40/100 - Loss: 3.2481\n","Epoch   9 Batch   50/100 - Loss: 3.5613\n","Epoch   9 Batch   60/100 - Loss: 3.9610\n","Epoch   9 Batch   70/100 - Loss: 4.2562\n","Epoch   9 Batch   80/100 - Loss: 3.4290\n","Epoch   9 Batch   90/100 - Loss: 3.3003\n","Model Trained and Saved\n"],"name":"stdout"}]}]}