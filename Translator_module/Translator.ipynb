{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Translator.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"-I5FuBphquhu","colab_type":"code","outputId":"f1e87499-7e03-4383-cd03-82f488a9664a","executionInfo":{"status":"ok","timestamp":1590509279268,"user_tz":-180,"elapsed":1689,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import translator_helper as helper\n","import os\n","import preprocess_util as putil\n","source_path = \"./raw_data/parallel_sentence_corpus/amh.txt\"\n","target_path = \"./raw_data/parallel_sentence_corpus/amh.txt\"\n","source_vocab_mapping = \"./amh_vocab_mapping.p\"\n","target_vocab_mapping = \"./amh_vocab_mapping.p\"\n","PREPROCESS_SAVE_PATH = \"preprocssed_data.p\"\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n",">>>>> This is L3Morpho, version 3.0 <<<<<\n",">>>>>  and HornMorpho, version 2.5  <<<<<\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9hKBrWLYquhz","colab_type":"code","colab":{}},"source":["src_int_to_vocab, src_vocab_to_int = putil.load_file(source_vocab_mapping)\n","tgt_int_to_vocab, tgt_vocab_to_int = putil.load_file(target_vocab_mapping)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"laRKfugpquh4","colab_type":"code","outputId":"d499cbf0-9b11-4f5a-f74a-d3d2047dedbb","executionInfo":{"status":"error","timestamp":1590509259538,"user_tz":-180,"elapsed":31801,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["helper.preprocess_and_save(source_path, target_path,\n","                           src_int_to_vocab, src_vocab_to_int,\n","                           tgt_int_to_vocab, tgt_vocab_to_int,\n","                           PREPROCESS_SAVE_PATH)\n","print(\"processed data saved...\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading morphological data for Amharic ...\n","\n"],"name":"stdout"},{"output_type":"stream","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","\n","KeyboardInterrupt\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vziHeYrtquh7","colab_type":"code","outputId":"ab82fa3b-f886-4930-8857-760599bbfe0b","executionInfo":{"status":"ok","timestamp":1590509076403,"user_tz":-180,"elapsed":959,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#checkpoint processed data saved"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["39770"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"NoAbnRnJquh-","colab_type":"code","colab":{}},"source":["import translator_helper as helper\n","import tensorflow as tf\n","import numpy as np\n","PREPROCESS_SAVE_PATH = \"preprocssed_data.p\"\n","(src_int_text, tgt_int_text), (src_int_to_vocab, tgt_int_to_vocab), (src_vocab_to_int, tgt_vocab_to_int) = helper.load_preprocessed_data(PREPROCESS_SAVE_PATH)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AKhw4V-jrXcP","colab_type":"code","colab":{}},"source":["!ln -sf ./drive/My\\ Drive/Amh2Amh_v3_checked/HornMorph/ ./"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVISvUVLquiC","colab_type":"code","outputId":"caec0b36-cee7-4ba1-a0ef-aaeb181bbb66","colab":{}},"source":["from tensorflow.python.layers.core import Dense\n","\n","print(tf.__version__)\n","\n","if not tf.test.gpu_device_name():\n","    print('No GPU found. Please use a GPU to train your neural network.')\n","else:\n","    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.14.0\n","No GPU found. Please use a GPU to train your neural network.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vbcRH_xOquiE","colab_type":"code","colab":{}},"source":["# prepare_model_input\n","def model_inputs():\n","    src_input = tf.placeholder(tf.int32, (None, None), name=\"src_input\")\n","    target = tf.placeholder(tf.int32, (None, None), name=\"targets\")\n","    learning_rate = tf.placeholder(tf.float32, [], name=\"learning_rate\")\n","    keep_prob = tf.placeholder(tf.float32, [], name=\"keep_prob\")\n","    src_seq_len = tf.placeholder(tf.int32, (None, ), name=\"src_seq_len\")\n","    tgt_seq_len = tf.placeholder(tf.int32, (None, ), name=\"tgt_seq_len\")\n","    max_tgt_seq = tf.reduce_max(tgt_seq_len)\n","    \n","    return src_input, target, learning_rate, keep_prob, src_seq_len, tgt_seq_len, max_tgt_seq"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEVYv4r8quiI","colab_type":"code","colab":{}},"source":["def prepare_decoder_input(target, target_to_int, batch_size):\n","    sliced = tf.strided_slice(target, [0,0], [batch_size, -1], [1,1])\n","    decoder_input = tf.concat([tf.fill([batch_size,1], target_to_int['<GO>']), sliced], 1)\n","    return decoder_input\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mL0Ppb4GquiK","colab_type":"code","colab":{}},"source":["def encoder(enc_inputs, src_seq_len, enc_embedding_size, src_vocab_size, rnn_size, num_layers, keep_prob=0.5):\n","    embed = tf.contrib.layers.embed_sequence(enc_inputs, src_vocab_size, enc_embedding_size)\n","    def build_cell(lstm_size, keep_prob):\n","        lstm = tf.contrib.rnn.LSTMCell(lstm_size)\n","        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n","        return drop\n","    cell = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n","    \n","    # returns output and state\n","    outputs, state = tf.nn.dynamic_rnn(cell, embed, sequence_length=src_seq_len, dtype=tf.float32)\n","    return outputs, state   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F48mjJgxquiN","colab_type":"code","colab":{}},"source":["def decoder_train(dec_embeded_input, enc_state, tgt_seq_len, max_tgt_len, dec_cell, keep_prob, output_layer):\n","    \n","    train_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embeded_input, \n","                                                     sequence_length=tgt_seq_len, \n","                                                     time_major=False)\n","    dec_train = tf.contrib.seq2seq.BasicDecoder(dec_cell, train_helper, enc_state, output_layer)\n","    \n","    decoder_train_output = tf.contrib.seq2seq.dynamic_decode(dec_train,\n","                                                       impute_finished=True,\n","                                                       maximum_iterations=max_tgt_len)[0]\n","    return decoder_train_output\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YE7akRM2quiP","colab_type":"code","colab":{}},"source":["def decoder_inference(dec_embedding_mat, enc_state, max_tgt_len, start_id, end_id, batch_size, dec_cell, keep_prob, output_layer):\n","    \n","    start_tokens = tf.tile(tf.constant([start_id], dtype=tf.int32), [batch_size], name='start_tokens')\n","    \n","    infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embedding_mat, start_tokens, end_id)\n","    \n","    dec_inference = tf.contrib.seq2seq.BasicDecoder(dec_cell, infer_helper, enc_state, output_layer)\n","    \n","    decoder_infer_output = tf.contrib.seq2seq.dynamic_decode(dec_inference,\n","                                                       impute_finished=True,\n","                                                       maximum_iterations=max_tgt_len)[0]\n","    return decoder_infer_output\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uIDKr2jHquiR","colab_type":"code","colab":{}},"source":["def decoder(dec_input, enc_state, \n","            tgt_seq_len, max_tgt_len, \n","            tgt_vocab_to_int, tgt_vocab_size, \n","            rnn_size, num_layers, dec_embed_size,\n","            batch_size, keep_prob):\n","    embed_mat = tf.Variable(tf.random_uniform([tgt_vocab_size, dec_embed_size]))\n","    dec_embeded_input = tf.nn.embedding_lookup(embed_mat, dec_input)\n","    \n","    def build_cell(lstm_size, keep_prob):\n","        lstm = tf.contrib.rnn.LSTMCell(lstm_size)\n","        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n","        return drop\n","    \n","    dec_cell = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n","    output_layer = Dense(tgt_vocab_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n","    \n","    with tf.variable_scope(\"decode\"):\n","        dec_train_output = decoder_train(dec_embeded_input, enc_state,\n","                                         tgt_seq_len, max_tgt_len,\n","                                         dec_cell, keep_prob, output_layer)\n","    start_id = tgt_vocab_to_int[\"<GO>\"]\n","    end_id = tgt_vocab_to_int[\"<EOS>\"]\n","   \n","    with tf.variable_scope(\"decode\", reuse=True):\n","        dec_infer_output = decoder_inference(embed_mat, enc_state, max_tgt_len,\n","                                                start_id, end_id, batch_size,\n","                                                dec_cell, keep_prob, output_layer)\n","    return dec_train_output, dec_infer_output\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Vdr4agZquiV","colab_type":"code","colab":{}},"source":["def EncoderDecoder(inputs, targets,\n","                   enc_embedding_size, dec_embedding_size,\n","                   src_seq_len, tgt_seq_len, max_tgt_len, \n","                   tgt_vocab_size, src_vocab_size, tgt_vocab_to_int,\n","                   rnn_size, num_layers, batch_size, keep_prob):\n","    \n","    outputs, state = encoder(inputs, src_seq_len, enc_embedding_size,\n","                      src_vocab_size, rnn_size, num_layers, keep_prob)\n","    \n","    dec_procssed_input = prepare_decoder_input(targets, tgt_vocab_to_int, batch_size)\n","    \n","    decoder_train_output, dec_infer_output = decoder(dec_procssed_input, state, tgt_seq_len, max_tgt_len,\n","                                                      tgt_vocab_to_int, tgt_vocab_size, rnn_size, num_layers,\n","                                                      dec_embedding_size, batch_size, keep_prob)\n","    return decoder_train_output, dec_infer_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XjJ9y21vquiX","colab_type":"code","colab":{}},"source":["def pad_sentences(batch, pad_id):\n","    \n","    seq_len = [len(sen) for sen in batch]\n","    #print(seq_len)\n","    max_len = max(seq_len)\n","        \n","    return [sent + [pad_id] * (max_len - len(sent)) for sent in batch]\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZAVv3mtquia","colab_type":"code","colab":{}},"source":["def getBatches(src_int_text, tgt_int_text, batch_size, src_pad_id, tgt_pad_id):\n","    assert len(src_int_text) == len(tgt_int_text), \"source sentences != target sentences\"\n","    n_batches = len(src_int_text) // batch_size\n","    \n","    total_sen_in_batch = batch_size * n_batches\n","    src_int_sents = src_int_text[:total_sen_in_batch + 1]\n","    tgt_int_sents = tgt_int_text[:total_sen_in_batch + 1]\n","    batches = []\n","    for idx in range(0, len(src_int_sents), batch_size):\n","        src_batch = src_int_sents[idx:idx + batch_size]\n","        tgt_batch = tgt_int_sents[idx:idx + batch_size]\n","        src_padded_batch = pad_sentences(src_batch, src_pad_id)\n","        tgt_padded_batch = pad_sentences(tgt_batch, tgt_pad_id)\n","        batches.append((src_padded_batch, tgt_padded_batch))\n","    return batches\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lco2tnYequic","colab_type":"code","colab":{}},"source":["train_graph = tf.Graph()\n","(src_int_text, tgt_int_text), (src_vocab_to_int, tgt_vocab_to_int), (src_int_to_vocab, tgt_int_to_vocab)  = helper.load_preprocessed_data(PREPROCESS_SAVE_PATH)\n","max_tgt_len = max([len(sen) for sen in src_int_text])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwoeYF0Nquif","colab_type":"code","colab":{}},"source":["epochs = 10\n","batch_size = 256\n","lr = 0.001\n","keep_probe = 0.75\n","rnn_size = 512\n","num_layers = 2\n","enc_embedding_size = 256\n","dec_embedding_size = 256\n","display_step = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sehzwaXTquih","colab_type":"code","outputId":"96b1b078-da31-4643-d691-7ff4064bd098","colab":{}},"source":["\n","with train_graph.as_default():\n","    src_inputs, targets, learning_rate, keep_prob, src_seq_len, tgt_seq_len, max_tgt_seq = model_inputs()\n","    train_logits, infer_logits = EncoderDecoder(src_inputs, targets,\n","                                                enc_embedding_size, dec_embedding_size,\n","                                                src_seq_len, tgt_seq_len, max_tgt_len, \n","                                                len(src_vocab_to_int), len(tgt_vocab_to_int), tgt_vocab_to_int,\n","                                                rnn_size, num_layers, batch_size, keep_prob)\n","    \n","    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n","    inference_logits = tf.identity(infer_logits.sample_id, name='predictions')\n","    masks = tf.sequence_mask(tgt_seq_len, max_tgt_seq, dtype=tf.float32, name='masks')\n","    \n","    with tf.name_scope(\"optimization\"):\n","        # Loss function\n","        cost = tf.contrib.seq2seq.sequence_loss(\n","            training_logits,\n","            targets,\n","            masks)\n","\n","        # Optimizer\n","        optimizer = tf.train.AdamOptimizer(learning_rate)\n","\n","        # Gradient Clipping\n","        gradients = optimizer.compute_gradients(cost)\n","        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n","        train_optimzer = optimizer.apply_gradients(capped_gradients)\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /Users/daniel/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From <ipython-input-5-5ac47e779404>:4: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-5-5ac47e779404>:7: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-5-5ac47e779404>:10: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /Users/daniel/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /Users/daniel/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0Yqzkotgquil","colab_type":"code","outputId":"ed010f7d-6e38-40e9-b781-15fd1a9271f2","colab":{}},"source":["with tf.Session(graph=train_graph) as sess:\n","    sess.run(tf.global_variables_initializer())\n","    src_pad_id = src_vocab_to_int['<PAD>']\n","    tgt_pad_id = tgt_vocab_to_int['<PAD>']\n","    batches = getBatches(src_int_text, tgt_int_text, batch_size, src_pad_id, tgt_pad_id)\n","    for epoch_i in range(epochs):\n","        for batch_ind, (src_batch, tgt_batch) in enumerate(batches):\n","            srce_seq_length = [len(sent) for sent in src_batch]\n","            tgte_seq_length = [len(sent) for sent in tgt_batch]\n","            _, loss = sess.run(\n","                [train_optimzer, cost],\n","                {src_inputs: src_batch,\n","                 targets: tgt_batch,\n","                 learning_rate: lr,\n","                 tgt_seq_len: tgte_seq_length,\n","                 src_seq_len: srce_seq_length,\n","                 keep_prob: keep_probe\n","                })\n","\n","\n","            if batch_ind % display_step == 0 and batch_ind > 0:\n","\n","\n","                batch_train_logits = sess.run(\n","                    inference_logits,\n","                    {src_inputs: src_batch,\n","                     src_seq_len: srce_seq_length,\n","                     tgt_seq_len: tgte_seq_length,\n","                     keep_prob: 1.0})\n","\n","                print('Epoch {:>3} Batch {:>4}/{} - Loss: {:>6.4f}'\n","                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, loss))\n","\n","    # Save Model\n","    saver = tf.train.Saver()\n","    saver.save(sess, save_path)\n","    print('Model Trained and Saved')"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-f38c11f1536f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                  \u001b[0mtgt_seq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtgte_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                  \u001b[0msrc_seq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msrce_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                  \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_probe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 })\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, op, message)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \"\"\"\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;34m\"\"\"Creates an `InvalidArgumentError`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     super(InvalidArgumentError, self).__init__(node_def, op, message,\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}