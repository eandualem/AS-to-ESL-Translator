{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Translator.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-I5FuBphquhu","colab_type":"code","colab":{}},"source":["import translator_helper as helper\n","import os\n","import preprocess_util as putil\n","source_path = \"./raw_data/parallel_sentence_corpus/amh.txt\"\n","target_path = \"./raw_data/parallel_sentence_corpus/amh.txt\"\n","source_vocab_mapping = \"./amh_vocab_mapping.p\"\n","target_vocab_mapping = \"./amh_vocab_mapping.p\"\n","PREPROCESS_SAVE_PATH = \"preprocssed_data.p\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HBEySQcWvRsR","colab_type":"code","colab":{}},"source":["#!ln -s ./drive/My\\ Drive/HornMorph/\n","#!ln -s ./drive/My\\ Drive/Amh2Amh_v3_checked/embedding_checkpoints/\n","#!ln -s  ./drive/My\\ Drive/Amh2Amh_v3_checked/translating_checkpoints/\n","#!ln -s ./drive/My\\ Drive/Amh2Amh_v3_checked/Translator_module2/* ./"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hKBrWLYquhz","colab_type":"code","colab":{}},"source":["src_int_to_vocab, src_vocab_to_int = putil.load_file(source_vocab_mapping)\n","tgt_int_to_vocab, tgt_vocab_to_int = putil.load_file(target_vocab_mapping)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"laRKfugpquh4","colab_type":"code","colab":{}},"source":["helper.preprocess_and_save(source_path, target_path,\n","                           src_int_to_vocab, src_vocab_to_int,\n","                           tgt_int_to_vocab, tgt_vocab_to_int,\n","                           PREPROCESS_SAVE_PATH)\n","print(\"processed data saved...\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vziHeYrtquh7","colab_type":"code","colab":{}},"source":["#checkpoint processed data saved"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NoAbnRnJquh-","colab_type":"code","colab":{}},"source":["import translator_helper as helper\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import preprocess_util as putil\n","import numpy as np\n","PREPROCESS_SAVE_PATH = \"preprocssed_data.p\"\n","(src_int_text, tgt_int_text), (src_int_to_vocab, tgt_int_to_vocab), (src_vocab_to_int, tgt_vocab_to_int) = helper.load_preprocessed_data(PREPROCESS_SAVE_PATH)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVISvUVLquiC","colab_type":"code","colab":{}},"source":["from tensorflow.python.layers.core import Dense\n","\n","print(tf.__version__)\n","\n","if not tf.test.gpu_device_name():\n","    print('No GPU found. Please use a GPU to train your neural network.')\n","else:\n","    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bX_U5mcLbqJr","colab_type":"code","colab":{}},"source":["def get_embedding_mat(embedding_size, vocab_size, load_path):\n","  embed_graph = tf.Graph()\n","  with embed_graph.as_default():\n","    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n","    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n","    embedding = tf.Variable(tf.truncated_normal([vocab_size, embedding_size]))\n","  with embed_graph.as_default():\n","    saver = tf.train.Saver()\n","\n","  with tf.Session(graph=embed_graph) as sess:\n","    saver.restore(sess, tf.train.latest_checkpoint(load_path))\n","    embed_mat = sess.run(embedding)\n","  #tf.reset_default_graph()\n","  return embed_mat"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vbcRH_xOquiE","colab_type":"code","colab":{}},"source":["# prepare_model_input\n","def model_inputs():\n","    src_input = tf.placeholder(tf.int32, (None, None), name=\"src_input\")\n","    target = tf.placeholder(tf.int32, (None, None), name=\"targets\")\n","    learning_rate = tf.placeholder(tf.float32, [], name=\"learning_rate\")\n","    keep_prob = tf.placeholder(tf.float32, [], name=\"keep_prob\")\n","    src_seq_len = tf.placeholder(tf.int32, (None, ), name=\"src_seq_len\")\n","    tgt_seq_len = tf.placeholder(tf.int32, (None, ), name=\"tgt_seq_len\")\n","    max_tgt_seq = tf.reduce_max(tgt_seq_len)\n","    \n","    return src_input, target, learning_rate, keep_prob, src_seq_len, tgt_seq_len, max_tgt_seq"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEVYv4r8quiI","colab_type":"code","colab":{}},"source":["def prepare_decoder_input(target, target_to_int, batch_size):\n","    sliced = tf.strided_slice(target, [0,0], [batch_size, -1], [1,1])\n","    decoder_input = tf.concat([tf.fill([batch_size,1], target_to_int['<GO>']), sliced], 1)\n","    return decoder_input\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mL0Ppb4GquiK","colab_type":"code","colab":{}},"source":["def encoder(enc_inputs, src_seq_len, enc_embedding_size, src_vocab_size, rnn_size, num_layers, keep_prob=0.5, embed_mat=None):\n","\n","    #embed = tf.contrib.layers.embed_sequence(enc_inputs, src_vocab_size, enc_embedding_size)\n","    #embed_mat = get_embedding_mat(enc_embedding_size, src_vocab_size, src_embed_load_path)\n","    embed = tf.nn.embedding_lookup(embed_mat, enc_inputs)\n","\n","    def build_cell(lstm_size, keep_prob):\n","        lstm = tf.contrib.rnn.LSTMCell(lstm_size)\n","        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n","        return drop\n","    cell = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n","    \n","    # returns output and state\n","    outputs, state = tf.nn.dynamic_rnn(cell, embed, sequence_length=src_seq_len, dtype=tf.float32)\n","    return outputs, state   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F48mjJgxquiN","colab_type":"code","colab":{}},"source":["def decoder_train(dec_embeded_input, enc_state, tgt_seq_len, max_tgt_len, dec_cell, keep_prob, output_layer):\n","    \n","    train_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embeded_input, \n","                                                     sequence_length=tgt_seq_len, \n","                                                     time_major=False)\n","    dec_train = tf.contrib.seq2seq.BasicDecoder(dec_cell, train_helper, enc_state, output_layer)\n","    \n","    decoder_train_output = tf.contrib.seq2seq.dynamic_decode(dec_train,\n","                                                       impute_finished=True,\n","                                                       maximum_iterations=max_tgt_len)[0]\n","    return decoder_train_output\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YE7akRM2quiP","colab_type":"code","colab":{}},"source":["def decoder_inference(dec_embedding_mat, enc_state, max_tgt_len, start_id, end_id, batch_size, dec_cell, keep_prob, output_layer):\n","    \n","    start_tokens = tf.tile(tf.constant([start_id], dtype=tf.int32), [batch_size], name='start_tokens')\n","    \n","    infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embedding_mat, start_tokens, end_id)\n","    \n","    dec_inference = tf.contrib.seq2seq.BasicDecoder(dec_cell, infer_helper, enc_state, output_layer)\n","    \n","    decoder_infer_output = tf.contrib.seq2seq.dynamic_decode(dec_inference,\n","                                                       impute_finished=True,\n","                                                       maximum_iterations=max_tgt_len)[0]\n","    return decoder_infer_output\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uIDKr2jHquiR","colab_type":"code","colab":{}},"source":["def decoder(dec_input, enc_state, \n","            tgt_seq_len, max_tgt_len, \n","            tgt_vocab_to_int, tgt_vocab_size, \n","            rnn_size, num_layers, dec_embed_size,\n","            batch_size, keep_prob, embed_mat=None):\n","    #embed_mat = tf.Variable(tf.random_uniform([tgt_vocab_size, dec_embed_size]))\n","    #embed_mat = get_embedding_mat(enc_embedding_size, tgt_vocab_size, tgt_embed_load_path)\n","    dec_embeded_input = tf.nn.embedding_lookup(embed_mat, dec_input)\n","    \n","    def build_cell(lstm_size, keep_prob):\n","        lstm = tf.contrib.rnn.LSTMCell(lstm_size)\n","        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n","        return drop\n","    \n","    dec_cell = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n","    output_layer = Dense(tgt_vocab_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n","    \n","    with tf.variable_scope(\"decode\"):\n","        dec_train_output = decoder_train(dec_embeded_input, enc_state,\n","                                         tgt_seq_len, max_tgt_len,\n","                                         dec_cell, keep_prob, output_layer)\n","    start_id = tgt_vocab_to_int[\"<GO>\"]\n","    end_id = tgt_vocab_to_int[\"<EOS>\"]\n","   \n","    with tf.variable_scope(\"decode\", reuse=True):\n","        dec_infer_output = decoder_inference(embed_mat, enc_state, max_tgt_len,\n","                                                start_id, end_id, batch_size,\n","                                                dec_cell, keep_prob, output_layer)\n","    return dec_train_output, dec_infer_output\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Vdr4agZquiV","colab_type":"code","colab":{}},"source":["def EncoderDecoder(inputs, targets,\n","                   enc_embedding_size, dec_embedding_size,\n","                   src_seq_len, tgt_seq_len, max_tgt_len, \n","                   tgt_vocab_size, src_vocab_size, tgt_vocab_to_int,\n","                   rnn_size, num_layers, batch_size, keep_prob,\n","                   src_embed_mat, tgt_embed_mat):\n","    \n","    outputs, state = encoder(inputs, src_seq_len, enc_embedding_size,\n","                      src_vocab_size, rnn_size, num_layers, keep_prob, src_embed_mat)\n","    \n","    dec_procssed_input = prepare_decoder_input(targets, tgt_vocab_to_int, batch_size)\n","    \n","    decoder_train_output, dec_infer_output = decoder(dec_procssed_input, state, tgt_seq_len, max_tgt_len,\n","                                                      tgt_vocab_to_int, tgt_vocab_size, rnn_size, num_layers,\n","                                                      dec_embedding_size, batch_size, keep_prob,tgt_embed_mat)\n","    return decoder_train_output, dec_infer_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XjJ9y21vquiX","colab_type":"code","colab":{}},"source":["def pad_sentence_batch(sentence_batch, pad_int):\n","    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n","    max_sentence = max([len(sentence) for sentence in sentence_batch])\n","    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZAVv3mtquia","colab_type":"code","colab":{}},"source":["def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n","    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n","    for batch_i in range(0, len(sources)//batch_size):\n","        start_i = batch_i * batch_size\n","\n","        # Slice the right amount for the batch\n","        sources_batch = sources[start_i:start_i + batch_size]\n","        targets_batch = targets[start_i:start_i + batch_size]\n","\n","        # Pad\n","        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n","        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n","\n","        # Need the lengths for the _lengths parameters\n","        pad_targets_lengths = []\n","        for target in pad_targets_batch:\n","            pad_targets_lengths.append(len(target))\n","\n","        pad_source_lengths = []\n","        for source in pad_sources_batch:\n","            pad_source_lengths.append(len(source))\n","\n","        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n","\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lco2tnYequic","colab_type":"code","colab":{}},"source":["tf.reset_default_graph()\n","train_graph = tf.Graph()\n","(src_int_text, tgt_int_text), (src_vocab_to_int, tgt_vocab_to_int), (src_int_to_vocab, tgt_int_to_vocab)  = helper.load_preprocessed_data(PREPROCESS_SAVE_PATH)\n","max_tgt_len = max([len(sen) for sen in tgt_int_text])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwoeYF0Nquif","colab_type":"code","colab":{}},"source":["epochs = 100\n","batch_size = 256\n","lr = 0.001\n","keep_probe = 0.75\n","rnn_size = 256\n","num_layers = 2\n","enc_embedding_size = 256\n","dec_embedding_size = 256\n","display_step = 10\n","src_embed_mat = get_embedding_mat(256, len(src_int_to_vocab),\"embedding_checkpoints\")\n","tgt_embed_mat = get_embedding_mat(256, len(src_int_to_vocab),\"embedding_checkpoints\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sehzwaXTquih","colab_type":"code","colab":{}},"source":["\n","with train_graph.as_default():\n","    src_inputs, targets, learning_rate, keep_prob, src_seq_len, tgt_seq_len, max_tgt_seq = model_inputs()\n","    train_logits, infer_logits = EncoderDecoder(src_inputs, targets,\n","                                                enc_embedding_size, dec_embedding_size,\n","                                                src_seq_len, tgt_seq_len, max_tgt_len, \n","                                                len(src_vocab_to_int), len(tgt_vocab_to_int), tgt_vocab_to_int,\n","                                                rnn_size, num_layers, batch_size, keep_prob,\n","                                                src_embed_mat, tgt_embed_mat)\n","    \n","    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n","    inference_logits = tf.identity(infer_logits.sample_id, name='predictions')\n","    masks = tf.sequence_mask(tgt_seq_len, max_tgt_seq, dtype=tf.float32, name='masks')\n","    \n","    with tf.name_scope(\"optimization\"):\n","        # Loss function\n","        cost = tf.contrib.seq2seq.sequence_loss(\n","            training_logits,\n","            targets,\n","            masks)\n","\n","        # Optimizer\n","        optimizer = tf.train.AdamOptimizer(learning_rate)\n","\n","        # Gradient Clipping\n","        gradients = optimizer.compute_gradients(cost)\n","        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n","        train_optimzer = optimizer.apply_gradients(capped_gradients)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Yqzkotgquil","colab_type":"code","outputId":"a438e47b-4b80-485b-a579-75511c92056f","executionInfo":{"status":"ok","timestamp":1590949472614,"user_tz":-180,"elapsed":1045755,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["save_path = \"./translating_checkpoints/dev\"\n","with tf.Session(graph=train_graph) as sess:\n","    sess.run(tf.global_variables_initializer())\n","    src_pad_id = src_vocab_to_int['<PAD>']\n","    tgt_pad_id = tgt_vocab_to_int['<PAD>']\n","    #batches = getBatches(src_int_text, tgt_int_text, batch_size, src_pad_id, tgt_pad_id)\n","    for epoch_i in range(epochs):\n","        for batch_ind, (src_batch,tgt_batch,srce_seq_length,tgte_seq_length) in enumerate(\n","            get_batches(src_int_text, tgt_int_text, batch_size, src_pad_id, tgt_pad_id)):\n","            #srce_seq_length = [len(sent) for sent in src_batch]\n","            #tgte_seq_length = [len(sent) for sent in tgt_batch]\n","            _, loss = sess.run(\n","                [train_optimzer, cost],\n","                {src_inputs: src_batch,\n","                 targets: tgt_batch,\n","                 learning_rate: lr,\n","                 tgt_seq_len: tgte_seq_length,\n","                 src_seq_len: srce_seq_length,\n","                 keep_prob: keep_probe\n","                })\n","\n","\n","            if batch_ind % display_step == 0 and batch_ind > 0:\n","\n","\n","                batch_train_logits = sess.run(\n","                    inference_logits,\n","                    {src_inputs: src_batch,\n","                     src_seq_len: srce_seq_length,\n","                     tgt_seq_len: tgte_seq_length,\n","                     keep_prob: 1.0})\n","\n","                print('Epoch {:>3} Batch {:>4}/{} - Loss: {:>6.4f}'\n","                      .format(epoch_i, batch_ind, len(src_int_text) // batch_size, loss))\n","\n","    # Save Model\n","    saver = tf.train.Saver()\n","    saver.save(sess, save_path)\n","    print('Model Trained and Saved')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Epoch   0 Batch   10/19 - Loss: 5.0884\n","Epoch   1 Batch   10/19 - Loss: 3.8877\n","Epoch   2 Batch   10/19 - Loss: 3.6246\n","Epoch   3 Batch   10/19 - Loss: 3.5304\n","Epoch   4 Batch   10/19 - Loss: 3.4929\n","Epoch   5 Batch   10/19 - Loss: 3.4731\n","Epoch   6 Batch   10/19 - Loss: 3.4368\n","Epoch   7 Batch   10/19 - Loss: 3.4251\n","Epoch   8 Batch   10/19 - Loss: 3.4064\n","Epoch   9 Batch   10/19 - Loss: 3.3975\n","Epoch  10 Batch   10/19 - Loss: 3.3735\n","Epoch  11 Batch   10/19 - Loss: 3.3408\n","Epoch  12 Batch   10/19 - Loss: 3.3061\n","Epoch  13 Batch   10/19 - Loss: 3.2891\n","Epoch  14 Batch   10/19 - Loss: 3.2886\n","Epoch  15 Batch   10/19 - Loss: 3.2482\n","Epoch  16 Batch   10/19 - Loss: 3.2240\n","Epoch  17 Batch   10/19 - Loss: 3.2025\n","Epoch  18 Batch   10/19 - Loss: 3.1872\n","Epoch  19 Batch   10/19 - Loss: 3.1578\n","Epoch  20 Batch   10/19 - Loss: 3.1441\n","Epoch  21 Batch   10/19 - Loss: 3.1329\n","Epoch  22 Batch   10/19 - Loss: 3.1019\n","Epoch  23 Batch   10/19 - Loss: 3.0655\n","Epoch  24 Batch   10/19 - Loss: 3.0412\n","Epoch  25 Batch   10/19 - Loss: 3.0310\n","Epoch  26 Batch   10/19 - Loss: 2.9958\n","Epoch  27 Batch   10/19 - Loss: 2.9729\n","Epoch  28 Batch   10/19 - Loss: 2.9324\n","Epoch  29 Batch   10/19 - Loss: 2.8971\n","Epoch  30 Batch   10/19 - Loss: 2.8655\n","Epoch  31 Batch   10/19 - Loss: 2.8303\n","Epoch  32 Batch   10/19 - Loss: 2.8013\n","Epoch  33 Batch   10/19 - Loss: 2.7639\n","Epoch  34 Batch   10/19 - Loss: 2.7537\n","Epoch  35 Batch   10/19 - Loss: 2.7342\n","Epoch  36 Batch   10/19 - Loss: 2.7042\n","Epoch  37 Batch   10/19 - Loss: 2.6744\n","Epoch  38 Batch   10/19 - Loss: 2.6600\n","Epoch  39 Batch   10/19 - Loss: 2.6317\n","Epoch  40 Batch   10/19 - Loss: 2.6195\n","Epoch  41 Batch   10/19 - Loss: 2.6146\n","Epoch  42 Batch   10/19 - Loss: 2.5506\n","Epoch  43 Batch   10/19 - Loss: 2.5394\n","Epoch  44 Batch   10/19 - Loss: 2.5174\n","Epoch  45 Batch   10/19 - Loss: 2.4714\n","Epoch  46 Batch   10/19 - Loss: 2.4295\n","Epoch  47 Batch   10/19 - Loss: 2.3982\n","Epoch  48 Batch   10/19 - Loss: 2.3749\n","Epoch  49 Batch   10/19 - Loss: 2.3405\n","Epoch  50 Batch   10/19 - Loss: 2.3109\n","Epoch  51 Batch   10/19 - Loss: 2.2834\n","Epoch  52 Batch   10/19 - Loss: 2.2540\n","Epoch  53 Batch   10/19 - Loss: 2.2236\n","Epoch  54 Batch   10/19 - Loss: 2.1946\n","Epoch  55 Batch   10/19 - Loss: 2.1645\n","Epoch  56 Batch   10/19 - Loss: 2.1477\n","Epoch  57 Batch   10/19 - Loss: 2.1096\n","Epoch  58 Batch   10/19 - Loss: 2.0911\n","Epoch  59 Batch   10/19 - Loss: 2.0574\n","Epoch  60 Batch   10/19 - Loss: 2.0276\n","Epoch  61 Batch   10/19 - Loss: 2.0118\n","Epoch  62 Batch   10/19 - Loss: 1.9873\n","Epoch  63 Batch   10/19 - Loss: 1.9521\n","Epoch  64 Batch   10/19 - Loss: 1.9261\n","Epoch  65 Batch   10/19 - Loss: 1.9080\n","Epoch  66 Batch   10/19 - Loss: 1.8772\n","Epoch  67 Batch   10/19 - Loss: 1.8452\n","Epoch  68 Batch   10/19 - Loss: 1.8122\n","Epoch  69 Batch   10/19 - Loss: 1.7948\n","Epoch  70 Batch   10/19 - Loss: 1.7560\n","Epoch  71 Batch   10/19 - Loss: 1.7421\n","Epoch  72 Batch   10/19 - Loss: 1.7180\n","Epoch  73 Batch   10/19 - Loss: 1.6964\n","Epoch  74 Batch   10/19 - Loss: 1.6695\n","Epoch  75 Batch   10/19 - Loss: 1.6495\n","Epoch  76 Batch   10/19 - Loss: 1.6369\n","Epoch  77 Batch   10/19 - Loss: 1.6080\n","Epoch  78 Batch   10/19 - Loss: 1.5740\n","Epoch  79 Batch   10/19 - Loss: 1.5546\n","Epoch  80 Batch   10/19 - Loss: 1.5275\n","Epoch  81 Batch   10/19 - Loss: 1.5094\n","Epoch  82 Batch   10/19 - Loss: 1.4860\n","Epoch  83 Batch   10/19 - Loss: 1.4587\n","Epoch  84 Batch   10/19 - Loss: 1.4445\n","Epoch  85 Batch   10/19 - Loss: 1.4153\n","Epoch  86 Batch   10/19 - Loss: 1.4018\n","Epoch  87 Batch   10/19 - Loss: 1.3793\n","Epoch  88 Batch   10/19 - Loss: 1.3560\n","Epoch  89 Batch   10/19 - Loss: 1.3338\n","Epoch  90 Batch   10/19 - Loss: 1.3144\n","Epoch  91 Batch   10/19 - Loss: 1.2936\n","Epoch  92 Batch   10/19 - Loss: 1.2832\n","Epoch  93 Batch   10/19 - Loss: 1.2633\n","Epoch  94 Batch   10/19 - Loss: 1.2614\n","Epoch  95 Batch   10/19 - Loss: 1.2357\n","Epoch  96 Batch   10/19 - Loss: 1.2078\n","Epoch  97 Batch   10/19 - Loss: 1.1853\n","Epoch  98 Batch   10/19 - Loss: 1.1849\n","Epoch  99 Batch   10/19 - Loss: 1.1485\n","Model Trained and Saved\n"],"name":"stdout"}]}]}